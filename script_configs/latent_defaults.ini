
[DEFAULTS]

#name of the run
name = test-latent-diffae

# training data directory
training_dir = /home/ubuntu/datasets/SignalTrain_LA2A_Dataset_1.1

# the batch size
batch_size = 8 

# number of GPUs to use for training
num_gpus = 1 

# number of nodes to use for training
num_nodes = 1 

# number of CPU workers for the DataLoader
num_workers = 12

# Number of audio samples for the training input
sample_size = 65536 

# Number of epochs between demos
demo_every = 200 

# Number of denoising steps for the demos       
demo_steps = 250

# Number of demos to create
num_demos = 16

# the random seed
seed = 42

# Batches for gradient accumulation
accum_batches = 1

# The sample rate of the audio
sample_rate = 48000   

# Number of steps between checkpoints
checkpoint_every = 10000                              

# the EMA decay
ema_decay = 0.995                           

# the validation set
latent_dim = 64            

# the validation set
codebook_size = 1024                 

# number of quantizers
num_quantizers = 0 

# number of residual quantizers (same as above, just depends on which file you're using)
num_residuals = 0 

# number of heads for the memcodes
num_heads = 8

# If true training data is kept in RAM
cache_training_data = False  

# number of sub-bands for the PQMF filter
pqmf_bands = 1

# randomly crop input audio? (for augmentation)
random_crop = True 

# normalize input audio?
norm_inputs = False

#Number of steps before freezing encoder in dVAE
warmup_steps = 100000

# checkpoint file to (re)start training from
ckpt_path = ''

# pretrained diffusion autoencoder checkpoint file
pretrained_ckpt_path = ''

# configuration model specifying model hyperparameters
model_config = ''

# directory to save the checkpoints in
save_dir = ''

#the multiprocessing start method ['fork', 'forkserver', 'spawn']
start_method = 'spawn'

preprocessed_dir = ''

depth=8

# Name of the run for automatic restarts
run_name = ''

# Checkpoint for a pre-trained CLAP model
clap_ckpt_path=''

clap_fusion=False

clap_amodel='HTSAT-tiny'